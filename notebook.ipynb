{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9682142,"sourceType":"datasetVersion","datasetId":5918268},{"sourceId":9699023,"sourceType":"datasetVersion","datasetId":5930736},{"sourceId":9699399,"sourceType":"datasetVersion","datasetId":5931047},{"sourceId":9839891,"sourceType":"datasetVersion","datasetId":6036282},{"sourceId":10204718,"sourceType":"datasetVersion","datasetId":6306240},{"sourceId":10207176,"sourceType":"datasetVersion","datasetId":6308100},{"sourceId":159901,"sourceType":"modelInstanceVersion","modelInstanceId":135943,"modelId":158667},{"sourceId":159916,"sourceType":"modelInstanceVersion","modelInstanceId":135958,"modelId":158681},{"sourceId":159950,"sourceType":"modelInstanceVersion","modelInstanceId":135987,"modelId":158711},{"sourceId":160194,"sourceType":"modelInstanceVersion","modelInstanceId":136207,"modelId":158923}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# data_set.py","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import Dataset\nimport os\nfrom PIL import Image\nimport json\n\n\nclass MyDataset(Dataset):\n    def __init__(self, args, mode, limit=None):\n        self.args = args\n        self.data = self.load_data(args, mode, limit)\n        self.image_ids = list(self.data.keys())  # Use unique indices as keys\n        for id in self.data.keys():\n            if mode in [\"train\"]:\n                self.data[id][\"image_path\"] = os.path.join(self.args.image_train, self.data[id][\"image\"])\n            else:\n                self.data[id][\"image_path\"] = os.path.join(self.args.image_test, self.data[id][\"image\"])\n    \n    def load_data(self, args, mode, limit=None):\n        cnt = 0\n        data_set = {}\n        label_mapping = {\n            \"not-sarcasm\": 0,\n            \"multi-sarcasm\": 1,\n            \"text-sarcasm\": 2,\n            \"image-sarcasm\": 3\n        }\n        \n        if mode in [\"train\"]:\n            with open(self.args.text_train, 'r', encoding='utf-8') as f:\n                datas = json.load(f)\n                for key, data in datas.items():\n                    if limit is not None and cnt >= limit:\n                        break\n\n                    file_name = data['image']\n                    sentence = data['caption']\n                    label = label_mapping[data['label']]\n                    \n                    cur_img_path = os.path.join(self.args.image_train, file_name)\n                    if not os.path.exists(cur_img_path):\n                        print(f\"{cur_img_path} not found!\")\n                        continue\n                    \n                    data_set[key] = {\n                        \"image\": file_name,\n                        \"caption\": sentence,\n                        \"label\": label\n                    }\n                    cnt += 1\n                    \n        elif mode in [\"test\"]:\n            with open(self.args.text_test, 'r', encoding='utf-8') as f:\n                datas = json.load(f)\n                for key, data in datas.items():\n                    file_name = data['image']\n                    sentence = data['caption']\n                    label = data['label']\n\n                    cur_img_path = os.path.join(self.args.image_test, file_name)\n                    if not os.path.exists(cur_img_path):\n                        print(f\"{cur_img_path} not found!\")\n                        continue\n                    \n                    data_set[key] = {\n                        \"image\": file_name,\n                        \"caption\": sentence,\n                        \"label\": label\n                    }\n                    cnt += 1\n                    \n        else:\n            print(\"Not found correct mode in MyDataset class!!!\")\n        \n        return data_set\n\n    def image_loader(self, id):\n        return Image.open(self.data[id][\"image_path\"])\n\n    def text_loader(self, id):\n        return self.data[id][\"caption\"]\n\n    def __getitem__(self, index):\n        id = self.image_ids[index]  # Access by unique key (index from JSON)\n        text = self.text_loader(id)\n        image_feature = self.image_loader(id)\n        label = self.data[id][\"label\"]\n        return text, image_feature, label, id\n\n    def __len__(self):\n        return len(self.image_ids)\n\n    @staticmethod\n    def collate_func(batch_data):\n        batch_size = len(batch_data)\n \n        if batch_size == 0:\n            return {}\n\n        text_list = []\n        image_list = []\n        label_list = []\n        id_list = []\n        for instance in batch_data:\n            text_list.append(instance[0])\n            image_list.append(instance[1])\n            label_list.append(instance[2])\n            id_list.append(instance[3])\n        return text_list, image_list, label_list, id_list\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-15T15:12:20.152476Z","iopub.execute_input":"2024-12-15T15:12:20.152863Z","iopub.status.idle":"2024-12-15T15:12:23.176335Z","shell.execute_reply.started":"2024-12-15T15:12:20.152831Z","shell.execute_reply":"2024-12-15T15:12:23.175438Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"# model.py","metadata":{}},{"cell_type":"code","source":"from transformers import CLIPModel,BertConfig\nfrom transformers.models.bert.modeling_bert import BertLayer\nimport torch.nn as nn\nimport torch\nimport torch.nn.functional as F\nimport copy\n\nclass MultimodalEncoder(nn.Module):\n    def __init__(self, config, layer_number):\n        super(MultimodalEncoder, self).__init__()\n        layer = BertLayer(config)\n        self.layer = nn.ModuleList([copy.deepcopy(layer) for _ in range(layer_number)])\n\n    def forward(self, hidden_states, attention_mask, output_all_encoded_layers=True):\n        all_encoder_layers = []\n        all_encoder_attentions = []\n        for layer_module in self.layer:\n            hidden_states, attention = layer_module(hidden_states, attention_mask, output_attentions=True)\n            all_encoder_attentions.append(attention)\n            if output_all_encoded_layers:\n                all_encoder_layers.append(hidden_states)\n        if not output_all_encoded_layers:\n            all_encoder_layers.append(hidden_states)\n        return all_encoder_layers, all_encoder_attentions\n\n\nclass MV_CLIP(nn.Module):\n    def __init__(self, args, class_weights=None):\n        super(MV_CLIP, self).__init__()\n        self.model = CLIPModel.from_pretrained(args.clip_model)\n        self.config = BertConfig.from_pretrained(\"bert-base-uncased\")\n        self.config.hidden_size = 512\n        self.config.num_attention_heads = 8\n        self.trans = MultimodalEncoder(self.config, layer_number=args.layers)\n        if args.simple_linear:\n            self.text_linear = nn.Linear(args.text_size, args.text_size)\n            self.image_linear = nn.Linear(args.image_size, args.image_size)\n        else:\n            self.text_linear =  nn.Sequential(\n                nn.Linear(args.text_size, args.text_size),\n                nn.Dropout(args.dropout_rate),\n                nn.GELU()\n            )\n            self.image_linear =  nn.Sequential(\n                nn.Linear(args.image_size, args.image_size),\n                nn.Dropout(args.dropout_rate),\n                nn.GELU()\n            )\n\n        self.classifier_fuse = nn.Linear(args.text_size , args.label_number)\n        self.classifier_text = nn.Linear(args.text_size, args.label_number)\n        self.classifier_image = nn.Linear(args.image_size, args.label_number)\n\n        self.loss_fct = nn.CrossEntropyLoss(weight=class_weights) # thêm class weight\n        self.att = nn.Linear(args.text_size, 1, bias=False)\n\n    def forward(self, inputs, labels):\n        output = self.model(**inputs,output_attentions=True)\n        text_features = output['text_model_output']['last_hidden_state']\n        image_features = output['vision_model_output']['last_hidden_state']\n        text_feature = output['text_model_output']['pooler_output']\n        image_feature = output['vision_model_output']['pooler_output']\n        text_feature = self.text_linear(text_feature)\n        image_feature = self.image_linear(image_feature)\n\n        text_embeds = self.model.text_projection(text_features)\n        image_embeds = self.model.visual_projection(image_features)\n        input_embeds = torch.cat((image_embeds, text_embeds), dim=1)\n        attention_mask = torch.cat((torch.ones(text_features.shape[0], 50).to(text_features.device), inputs['attention_mask']), dim=-1) # patch 14 thì thay từ 50 sang 257\n        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n        extended_attention_mask = extended_attention_mask.to(dtype=next(self.parameters()).dtype)\n        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n        fuse_hiddens, all_attentions = self.trans(input_embeds, extended_attention_mask, output_all_encoded_layers=False)\n        fuse_hiddens = fuse_hiddens[-1]\n        new_text_features = fuse_hiddens[:, 50:, :]\n        new_text_feature = new_text_features[\n            torch.arange(new_text_features.shape[0], device=inputs['input_ids'].device), inputs['input_ids'].to(torch.int).argmax(dim=-1)\n        ]\n\n        new_image_feature = fuse_hiddens[:, 0, :].squeeze(1)\n\n        text_weight = self.att(new_text_feature)\n        image_weight = self.att(new_image_feature)    \n        att = nn.functional.softmax(torch.stack((text_weight, image_weight), dim=-1),dim=-1)\n        tw, iw = att.split([1,1], dim=-1)\n        fuse_feature = tw.squeeze(1) * new_text_feature + iw.squeeze(1) * new_image_feature\n\n        logits_fuse = self.classifier_fuse(fuse_feature)\n        logits_text = self.classifier_text(text_feature)\n        logits_image = self.classifier_image(image_feature)\n   \n        fuse_score = nn.functional.softmax(logits_fuse, dim=-1)\n        text_score = nn.functional.softmax(logits_text, dim=-1)\n        image_score = nn.functional.softmax(logits_image, dim=-1)\n\n        score = fuse_score + text_score + image_score\n\n        outputs = (score,)\n        if labels is not None:\n            loss_fuse = self.loss_fct(logits_fuse, labels)\n            loss_text = self.loss_fct(logits_text, labels)\n            loss_image = self.loss_fct(logits_image, labels)\n            loss = loss_fuse + loss_text + loss_image\n\n            outputs = (loss,) + outputs\n        return outputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T15:12:23.178319Z","iopub.execute_input":"2024-12-15T15:12:23.178885Z","iopub.status.idle":"2024-12-15T15:12:25.306756Z","shell.execute_reply.started":"2024-12-15T15:12:23.178839Z","shell.execute_reply":"2024-12-15T15:12:25.305837Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# train.py\n","metadata":{}},{"cell_type":"code","source":"import os\nfrom torch.utils.data import DataLoader\nimport torch\nfrom tqdm import tqdm, trange\nfrom sklearn import metrics\nimport numpy as np\n\n\n# def train(args, model, device, train_data, dev_data, processor):\ndef train(args, model, device, train_data, processor):\n    if not os.path.exists(args.output_dir):\n        os.mkdir(args.output_dir)\n\n    train_loader = DataLoader(dataset=train_data,\n                              batch_size=args.train_batch_size,\n                              collate_fn=MyDataset.collate_func,\n                              shuffle=True)\n    total_steps = int(len(train_loader) * args.num_train_epochs)\n    \n    model.to(device)\n\n    if args.optimizer_name == 'adafactor':\n        from transformers.optimization import Adafactor, AdafactorSchedule\n\n        print('Use Adafactor Optimizer for Training.')\n        optimizer = Adafactor(\n            model.parameters(),\n            # lr=1e-3,\n            # eps=(1e-30, 1e-3),\n            # clip_threshold=1.0,\n            # decay_rate=-0.8,\n            # beta1=None,\n            lr=None,\n            weight_decay=args.weight_decay,\n            relative_step=True,\n            scale_parameter=True,\n            warmup_init=True\n        )\n        scheduler = AdafactorSchedule(optimizer)\n    elif args.optimizer_name == 'adam':\n        print('Use AdamW Optimizer for Training.')\n        from transformers.optimization import AdamW, get_linear_schedule_with_warmup\n        if args.model == 'MV_CLIP':\n            clip_params = list(map(id, model.model.parameters()))\n            base_params = filter(lambda p: id(p) not in clip_params, model.parameters())\n            optimizer = AdamW([\n                    {\"params\": base_params},\n                    {\"params\": model.model.parameters(),\"lr\": args.clip_learning_rate}\n                    ], lr=args.learning_rate, weight_decay=args.weight_decay)\n\n            scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=int(args.warmup_proportion * total_steps),\n                                                    num_training_steps=total_steps)\n        else:\n            optimizer = AdamW(model.parameters(), lr=args.learning_rate, eps=args.adam_epsilon, weight_decay=args.weight_decay)\n            scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=int(args.warmup_proportion * total_steps),\n                                                num_training_steps=total_steps)\n\n    else:\n        raise Exception('Wrong Optimizer Name!!!')\n\n\n    max_acc = 0.\n    for i_epoch in trange(0, int(args.num_train_epochs), desc=\"Epoch\", disable=False):\n        sum_loss = 0.\n        sum_step = 0\n\n        iter_bar = tqdm(train_loader, desc=\"Iter (loss=X.XXX)\", disable=False)\n        model.train()\n\n        for step, batch in enumerate(iter_bar):\n            text_list, image_list, label_list, id_list = batch\n            if args.model == 'MV_CLIP':\n                inputs = processor(text=text_list, images=image_list, padding='max_length', truncation=True, max_length=args.max_len, return_tensors=\"pt\").to(device)\n                labels = torch.tensor(label_list).to(device)\n\n            loss, score = model(inputs,labels=labels)\n            sum_loss += loss.item()\n            sum_step += 1\n\n            iter_bar.set_description(\"Iter (loss=%5.3f)\" % loss.item())\n            loss.backward()\n            optimizer.step()\n            if args.optimizer_name == 'adam':\n                scheduler.step() \n            optimizer.zero_grad()\n\n        print(f\"Epoch {i_epoch + 1}\")\n        print(f\"Train loss {sum_loss/sum_step}\")\n\n        \n        path_to_save = os.path.join(args.output_dir, args.model)\n        if not os.path.exists(path_to_save):\n            os.mkdir(path_to_save)\n        model_to_save = (model.module if hasattr(model, \"module\") else model)\n        torch.save(model_to_save.state_dict(), os.path.join(path_to_save, f'model{args.current_epoch+i_epoch}.pt'))\n        print(f\"Saved model at {os.path.join(path_to_save, f'model{args.current_epoch+i_epoch}{args.attempt}.pt')}\")\n\n            \n        torch.cuda.empty_cache()\n    print('Train done')","metadata":{"execution":{"iopub.status.busy":"2024-12-15T15:12:25.307951Z","iopub.execute_input":"2024-12-15T15:12:25.308417Z","iopub.status.idle":"2024-12-15T15:12:26.526858Z","shell.execute_reply.started":"2024-12-15T15:12:25.308380Z","shell.execute_reply":"2024-12-15T15:12:26.525726Z"},"trusted":true},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"# main.py","metadata":{}},{"cell_type":"code","source":"import os\n# os.environ[\"CUDA_VISIBLE_DEVICES\"] = '2'\nimport torch\nimport argparse\nimport random\nimport numpy as np\nfrom transformers import CLIPProcessor, AutoTokenizer, AutoProcessor\nimport pickle\nfrom PIL import ImageFile\nfrom sklearn.model_selection import train_test_split\nImageFile.LOAD_TRUNCATED_IMAGES = True\n\n\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n\n    \ndef compute_class_weights(args, train_data):\n    # Count the number of occurrences of each class\n    class_counts = torch.zeros(args.label_number)\n    \n    for data in train_data:\n        _, _, label, _ = data\n        class_counts[label] += 1\n    \n    # Compute class weights\n    total_samples = class_counts.sum().item()\n    class_weights = total_samples / (class_counts * len(class_counts))\n\n    return class_weights\n    \n    \nclass Args:\n    def __init__(self, **entries):\n        self.__dict__.update(entries)\n        \n\ndef main():\n    args_dict = {\n        'device': '0',\n        'model': 'MV_CLIP',\n        'text_train': '/kaggle/input/hateam-processed-en/HAteam_processed_en.json',\n        'image_train': '/kaggle/input/muiltimodal-sarcasm/data/training-images',\n        'simple_linear': False,\n        'num_train_epochs': 4,\n        'train_batch_size': 32,\n        'label_number': 4,\n        'text_size': 512,\n        'image_size': 768,\n        'adam_epsilon': 1e-8,\n        'optimizer_name': 'adam',\n        'learning_rate': 5e-4,\n        'clip_learning_rate': 1e-6,\n        'max_len': 77,\n        'layers': 6,\n        'max_grad_norm': 5.0,\n        'weight_decay': 0.05,\n        'warmup_proportion': 0.2,\n        'dropout_rate': 0.1,\n        'output_dir': '/kaggle/working/',\n        'limit': None,\n        'seed': 42,\n        'model_path': '/kaggle/working/MV_CLIP',\n        'clip_model': 'openai/clip-vit-base-patch32',\n        'current_epoch': 9,\n        'attempt': ''\n    }\n    \n    args = Args(**args_dict)\n    \n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    seed_everything(args.seed)\n\n    # Load full training data và test data\n    train_data = MyDataset(args, mode='train', limit=None)\n    \n    # Compute class weights\n    class_weights = compute_class_weights(args, train_data)\n    class_weights = class_weights.to(device)  # Move to same device as model\n    \n    if args.model == 'MV_CLIP':\n        processor = CLIPProcessor.from_pretrained(args.clip_model)\n        model = MV_CLIP(args, class_weights=class_weights)\n    else:\n        raise RuntimeError('Error model name!')\n\n    #model.load_state_dict(torch.load('/kaggle/working/MV_CLIP/model4.pt', map_location=\"cpu\"))\n    model.to(device)\n\n    train(args, model, device, train_data, processor)","metadata":{"execution":{"iopub.status.busy":"2024-12-15T15:12:26.528847Z","iopub.execute_input":"2024-12-15T15:12:26.529882Z","iopub.status.idle":"2024-12-15T15:12:39.135960Z","shell.execute_reply.started":"2024-12-15T15:12:26.529838Z","shell.execute_reply":"2024-12-15T15:12:39.135042Z"},"trusted":true},"outputs":[],"execution_count":4},{"cell_type":"code","source":"main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T15:12:39.136964Z","iopub.execute_input":"2024-12-15T15:12:39.137457Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f023a79604184ec9adfdd9f4d3ee6b86"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/592 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d381f50d761a47cb8e5cf9fbf71f1e77"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/862k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bfbcf6b32f2e4ea0b1fd149c024fffd8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/525k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"653eb1ea7de44bbcb7f7633b1a428067"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.22M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"66553cc98ac3460294cb54247191bd32"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/389 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a7a75926cf084d1285ef05eaf3628224"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/4.19k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5fa7738615f448ab81583b699caa6960"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/605M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d181b100a22a43a792b6ed3f97de081d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b0eabbd58c784894882cf9f42f2e5164"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Use AdamW Optimizer for Training.\n","output_type":"stream"},{"name":"stderr","text":"Epoch:   0%|          | 0/4 [00:00<?, ?it/s]\nIter (loss=X.XXX):   0%|          | 0/478 [00:00<?, ?it/s]\u001b[ACLIPModel is using CLIPSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n\nIter (loss=4.284):   0%|          | 0/478 [00:01<?, ?it/s]\u001b[A\nIter (loss=4.284):   0%|          | 1/478 [00:01<14:29,  1.82s/it]\u001b[A\nIter (loss=4.263):   0%|          | 1/478 [00:02<14:29,  1.82s/it]\u001b[A\nIter (loss=4.263):   0%|          | 2/478 [00:02<09:45,  1.23s/it]\u001b[A\nIter (loss=4.247):   0%|          | 2/478 [00:03<09:45,  1.23s/it]\u001b[A\nIter (loss=4.247):   1%|          | 3/478 [00:03<08:07,  1.03s/it]\u001b[A\nIter (loss=4.179):   1%|          | 3/478 [00:03<08:07,  1.03s/it]\u001b[A\nIter (loss=4.179):   1%|          | 4/478 [00:04<07:30,  1.05it/s]\u001b[A\nIter (loss=4.240):   1%|          | 4/478 [00:04<07:30,  1.05it/s]\u001b[A\nIter (loss=4.240):   1%|          | 5/478 [00:05<06:54,  1.14it/s]\u001b[A\nIter (loss=4.264):   1%|          | 5/478 [00:05<06:54,  1.14it/s]\u001b[A\nIter (loss=4.264):   1%|▏         | 6/478 [00:05<06:35,  1.19it/s]\u001b[A\nIter (loss=4.252):   1%|▏         | 6/478 [00:06<06:35,  1.19it/s]\u001b[A\nIter (loss=4.252):   1%|▏         | 7/478 [00:06<06:31,  1.20it/s]\u001b[A\nIter (loss=4.165):   1%|▏         | 7/478 [00:07<06:31,  1.20it/s]\u001b[A\nIter (loss=4.165):   2%|▏         | 8/478 [00:07<06:19,  1.24it/s]\u001b[A\nIter (loss=4.147):   2%|▏         | 8/478 [00:07<06:19,  1.24it/s]\u001b[A\nIter (loss=4.147):   2%|▏         | 9/478 [00:08<06:13,  1.26it/s]\u001b[A\nIter (loss=4.355):   2%|▏         | 9/478 [00:08<06:13,  1.26it/s]\u001b[A\nIter (loss=4.355):   2%|▏         | 10/478 [00:08<06:06,  1.28it/s]\u001b[A\nIter (loss=4.292):   2%|▏         | 10/478 [00:09<06:06,  1.28it/s]\u001b[A\nIter (loss=4.292):   2%|▏         | 11/478 [00:09<06:04,  1.28it/s]\u001b[A\nIter (loss=4.208):   2%|▏         | 11/478 [00:10<06:04,  1.28it/s]\u001b[A\nIter (loss=4.208):   3%|▎         | 12/478 [00:10<06:01,  1.29it/s]\u001b[A\nIter (loss=4.072):   3%|▎         | 12/478 [00:10<06:01,  1.29it/s]\u001b[A\nIter (loss=4.072):   3%|▎         | 13/478 [00:11<05:58,  1.30it/s]\u001b[A\nIter (loss=4.282):   3%|▎         | 13/478 [00:11<05:58,  1.30it/s]\u001b[A\nIter (loss=4.282):   3%|▎         | 14/478 [00:11<05:55,  1.31it/s]\u001b[A\nIter (loss=4.127):   3%|▎         | 14/478 [00:12<05:55,  1.31it/s]\u001b[A\nIter (loss=4.127):   3%|▎         | 15/478 [00:12<05:54,  1.31it/s]\u001b[A\nIter (loss=4.086):   3%|▎         | 15/478 [00:13<05:54,  1.31it/s]\u001b[A\nIter (loss=4.086):   3%|▎         | 16/478 [00:13<05:53,  1.31it/s]\u001b[A\nIter (loss=4.049):   3%|▎         | 16/478 [00:13<05:53,  1.31it/s]\u001b[A\nIter (loss=4.049):   4%|▎         | 17/478 [00:14<05:54,  1.30it/s]\u001b[A\nIter (loss=4.224):   4%|▎         | 17/478 [00:14<05:54,  1.30it/s]\u001b[A\nIter (loss=4.224):   4%|▍         | 18/478 [00:14<05:51,  1.31it/s]\u001b[A\nIter (loss=4.209):   4%|▍         | 18/478 [00:15<05:51,  1.31it/s]\u001b[A\nIter (loss=4.209):   4%|▍         | 19/478 [00:15<05:52,  1.30it/s]\u001b[A\nIter (loss=4.016):   4%|▍         | 19/478 [00:16<05:52,  1.30it/s]\u001b[A\nIter (loss=4.016):   4%|▍         | 20/478 [00:16<05:57,  1.28it/s]\u001b[A\nIter (loss=4.194):   4%|▍         | 20/478 [00:17<05:57,  1.28it/s]\u001b[A\nIter (loss=4.194):   4%|▍         | 21/478 [00:17<05:54,  1.29it/s]\u001b[A\nIter (loss=4.118):   4%|▍         | 21/478 [00:17<05:54,  1.29it/s]\u001b[A\nIter (loss=4.118):   5%|▍         | 22/478 [00:18<05:53,  1.29it/s]\u001b[A\nIter (loss=4.277):   5%|▍         | 22/478 [00:18<05:53,  1.29it/s]\u001b[A\nIter (loss=4.277):   5%|▍         | 23/478 [00:18<05:54,  1.28it/s]\u001b[A\nIter (loss=4.163):   5%|▍         | 23/478 [00:19<05:54,  1.28it/s]\u001b[A\nIter (loss=4.163):   5%|▌         | 24/478 [00:19<05:56,  1.27it/s]\u001b[A\nIter (loss=4.054):   5%|▌         | 24/478 [00:20<05:56,  1.27it/s]\u001b[A\nIter (loss=4.054):   5%|▌         | 25/478 [00:20<05:53,  1.28it/s]\u001b[A\nIter (loss=4.167):   5%|▌         | 25/478 [00:20<05:53,  1.28it/s]\u001b[A\nIter (loss=4.167):   5%|▌         | 26/478 [00:21<05:50,  1.29it/s]\u001b[A\nIter (loss=3.875):   5%|▌         | 26/478 [00:21<05:50,  1.29it/s]\u001b[A\nIter (loss=3.875):   6%|▌         | 27/478 [00:22<05:50,  1.29it/s]\u001b[A\nIter (loss=4.016):   6%|▌         | 27/478 [00:22<05:50,  1.29it/s]\u001b[A\nIter (loss=4.016):   6%|▌         | 28/478 [00:22<05:49,  1.29it/s]\u001b[A\nIter (loss=3.917):   6%|▌         | 28/478 [00:23<05:49,  1.29it/s]\u001b[A\nIter (loss=3.917):   6%|▌         | 29/478 [00:23<05:47,  1.29it/s]\u001b[A\nIter (loss=3.980):   6%|▌         | 29/478 [00:24<05:47,  1.29it/s]\u001b[A\nIter (loss=3.980):   6%|▋         | 30/478 [00:24<05:46,  1.29it/s]\u001b[A\nIter (loss=4.347):   6%|▋         | 30/478 [00:24<05:46,  1.29it/s]\u001b[A\nIter (loss=4.347):   6%|▋         | 31/478 [00:25<05:42,  1.30it/s]\u001b[A\nIter (loss=3.832):   6%|▋         | 31/478 [00:25<05:42,  1.30it/s]\u001b[A\nIter (loss=3.832):   7%|▋         | 32/478 [00:25<05:42,  1.30it/s]\u001b[A\nIter (loss=3.884):   7%|▋         | 32/478 [00:26<05:42,  1.30it/s]\u001b[A\nIter (loss=3.884):   7%|▋         | 33/478 [00:26<05:44,  1.29it/s]\u001b[A\nIter (loss=3.897):   7%|▋         | 33/478 [00:27<05:44,  1.29it/s]\u001b[A\nIter (loss=3.897):   7%|▋         | 34/478 [00:27<05:44,  1.29it/s]\u001b[A\nIter (loss=3.984):   7%|▋         | 34/478 [00:27<05:44,  1.29it/s]\u001b[A\nIter (loss=3.984):   7%|▋         | 35/478 [00:28<05:42,  1.29it/s]\u001b[A\nIter (loss=4.120):   7%|▋         | 35/478 [00:28<05:42,  1.29it/s]\u001b[A\nIter (loss=4.120):   8%|▊         | 36/478 [00:28<05:40,  1.30it/s]\u001b[A\nIter (loss=4.153):   8%|▊         | 36/478 [00:29<05:40,  1.30it/s]\u001b[A\nIter (loss=4.153):   8%|▊         | 37/478 [00:29<05:37,  1.31it/s]\u001b[A\nIter (loss=4.386):   8%|▊         | 37/478 [00:30<05:37,  1.31it/s]\u001b[A\nIter (loss=4.386):   8%|▊         | 38/478 [00:30<05:41,  1.29it/s]\u001b[A\nIter (loss=4.084):   8%|▊         | 38/478 [00:31<05:41,  1.29it/s]\u001b[A\nIter (loss=4.084):   8%|▊         | 39/478 [00:31<05:43,  1.28it/s]\u001b[A\nIter (loss=4.510):   8%|▊         | 39/478 [00:31<05:43,  1.28it/s]\u001b[A\nIter (loss=4.510):   8%|▊         | 40/478 [00:32<05:38,  1.29it/s]\u001b[A\nIter (loss=4.343):   8%|▊         | 40/478 [00:32<05:38,  1.29it/s]\u001b[A\nIter (loss=4.343):   9%|▊         | 41/478 [00:32<05:32,  1.31it/s]\u001b[A\nIter (loss=4.543):   9%|▊         | 41/478 [00:33<05:32,  1.31it/s]\u001b[A\nIter (loss=4.543):   9%|▉         | 42/478 [00:33<05:28,  1.33it/s]\u001b[A\nIter (loss=4.212):   9%|▉         | 42/478 [00:34<05:28,  1.33it/s]\u001b[A\nIter (loss=4.212):   9%|▉         | 43/478 [00:34<05:28,  1.33it/s]\u001b[A\nIter (loss=4.165):   9%|▉         | 43/478 [00:34<05:28,  1.33it/s]\u001b[A\nIter (loss=4.165):   9%|▉         | 44/478 [00:35<05:28,  1.32it/s]\u001b[A\nIter (loss=4.167):   9%|▉         | 44/478 [00:35<05:28,  1.32it/s]\u001b[A\nIter (loss=4.167):   9%|▉         | 45/478 [00:35<05:26,  1.33it/s]\u001b[A\nIter (loss=3.870):   9%|▉         | 45/478 [00:36<05:26,  1.33it/s]\u001b[A\nIter (loss=3.870):  10%|▉         | 46/478 [00:36<05:36,  1.29it/s]\u001b[A\nIter (loss=4.430):  10%|▉         | 46/478 [00:37<05:36,  1.29it/s]\u001b[A\nIter (loss=4.430):  10%|▉         | 47/478 [00:37<05:41,  1.26it/s]\u001b[A\nIter (loss=4.250):  10%|▉         | 47/478 [00:37<05:41,  1.26it/s]\u001b[A\nIter (loss=4.250):  10%|█         | 48/478 [00:38<05:40,  1.26it/s]\u001b[A\nIter (loss=3.957):  10%|█         | 48/478 [00:38<05:40,  1.26it/s]\u001b[A\nIter (loss=3.957):  10%|█         | 49/478 [00:38<05:36,  1.28it/s]\u001b[A\nIter (loss=4.258):  10%|█         | 49/478 [00:39<05:36,  1.28it/s]\u001b[A\nIter (loss=4.258):  10%|█         | 50/478 [00:39<05:34,  1.28it/s]\u001b[A\nIter (loss=3.997):  10%|█         | 50/478 [00:40<05:34,  1.28it/s]\u001b[A\nIter (loss=3.997):  11%|█         | 51/478 [00:40<05:34,  1.28it/s]\u001b[A\nIter (loss=4.057):  11%|█         | 51/478 [00:41<05:34,  1.28it/s]\u001b[A\nIter (loss=4.057):  11%|█         | 52/478 [00:41<05:30,  1.29it/s]\u001b[A\nIter (loss=4.168):  11%|█         | 52/478 [00:41<05:30,  1.29it/s]\u001b[A\nIter (loss=4.168):  11%|█         | 53/478 [00:42<05:30,  1.29it/s]\u001b[A\nIter (loss=3.881):  11%|█         | 53/478 [00:42<05:30,  1.29it/s]\u001b[A\nIter (loss=3.881):  11%|█▏        | 54/478 [00:42<05:30,  1.28it/s]\u001b[A\nIter (loss=4.009):  11%|█▏        | 54/478 [00:43<05:30,  1.28it/s]\u001b[A\nIter (loss=4.009):  12%|█▏        | 55/478 [00:43<05:30,  1.28it/s]\u001b[A\nIter (loss=3.972):  12%|█▏        | 55/478 [00:44<05:30,  1.28it/s]\u001b[A\nIter (loss=3.972):  12%|█▏        | 56/478 [00:44<05:28,  1.29it/s]\u001b[A\nIter (loss=3.978):  12%|█▏        | 56/478 [00:44<05:28,  1.29it/s]\u001b[A\nIter (loss=3.978):  12%|█▏        | 57/478 [00:45<05:27,  1.28it/s]\u001b[A\nIter (loss=4.145):  12%|█▏        | 57/478 [00:45<05:27,  1.28it/s]\u001b[A\nIter (loss=4.145):  12%|█▏        | 58/478 [00:46<05:31,  1.27it/s]\u001b[A\nIter (loss=4.047):  12%|█▏        | 58/478 [00:46<05:31,  1.27it/s]\u001b[A\nIter (loss=4.047):  12%|█▏        | 59/478 [00:46<05:27,  1.28it/s]\u001b[A\nIter (loss=4.160):  12%|█▏        | 59/478 [00:47<05:27,  1.28it/s]\u001b[A\nIter (loss=4.160):  13%|█▎        | 60/478 [00:47<05:27,  1.28it/s]\u001b[A\nIter (loss=3.924):  13%|█▎        | 60/478 [00:48<05:27,  1.28it/s]\u001b[A\nIter (loss=3.924):  13%|█▎        | 61/478 [00:48<05:26,  1.28it/s]\u001b[A\nIter (loss=4.073):  13%|█▎        | 61/478 [00:48<05:26,  1.28it/s]\u001b[A\nIter (loss=4.073):  13%|█▎        | 62/478 [00:49<05:25,  1.28it/s]\u001b[A\nIter (loss=4.113):  13%|█▎        | 62/478 [00:49<05:25,  1.28it/s]\u001b[A\nIter (loss=4.113):  13%|█▎        | 63/478 [00:49<05:24,  1.28it/s]\u001b[A\nIter (loss=3.983):  13%|█▎        | 63/478 [00:50<05:24,  1.28it/s]\u001b[A\nIter (loss=3.983):  13%|█▎        | 64/478 [00:50<05:22,  1.28it/s]\u001b[A\nIter (loss=4.076):  13%|█▎        | 64/478 [00:51<05:22,  1.28it/s]\u001b[A\nIter (loss=4.076):  14%|█▎        | 65/478 [00:51<05:20,  1.29it/s]\u001b[A\nIter (loss=3.969):  14%|█▎        | 65/478 [00:51<05:20,  1.29it/s]\u001b[A\nIter (loss=3.969):  14%|█▍        | 66/478 [00:52<05:21,  1.28it/s]\u001b[A\nIter (loss=3.957):  14%|█▍        | 66/478 [00:52<05:21,  1.28it/s]\u001b[A\nIter (loss=3.957):  14%|█▍        | 67/478 [00:53<05:21,  1.28it/s]\u001b[A\nIter (loss=3.946):  14%|█▍        | 67/478 [00:53<05:21,  1.28it/s]\u001b[A\nIter (loss=3.946):  14%|█▍        | 68/478 [00:53<05:19,  1.28it/s]\u001b[A\nIter (loss=4.259):  14%|█▍        | 68/478 [00:54<05:19,  1.28it/s]\u001b[A\nIter (loss=4.259):  14%|█▍        | 69/478 [00:54<05:18,  1.28it/s]\u001b[A\nIter (loss=4.114):  14%|█▍        | 69/478 [00:55<05:18,  1.28it/s]\u001b[A\nIter (loss=4.114):  15%|█▍        | 70/478 [00:55<05:19,  1.28it/s]\u001b[A\nIter (loss=3.868):  15%|█▍        | 70/478 [00:55<05:19,  1.28it/s]\u001b[A\nIter (loss=3.868):  15%|█▍        | 71/478 [00:56<05:18,  1.28it/s]\u001b[A\nIter (loss=4.177):  15%|█▍        | 71/478 [00:56<05:18,  1.28it/s]\u001b[A\nIter (loss=4.177):  15%|█▌        | 72/478 [00:56<05:18,  1.27it/s]\u001b[A\nIter (loss=3.871):  15%|█▌        | 72/478 [00:57<05:18,  1.27it/s]\u001b[A\nIter (loss=3.871):  15%|█▌        | 73/478 [00:57<05:18,  1.27it/s]\u001b[A\nIter (loss=3.776):  15%|█▌        | 73/478 [00:58<05:18,  1.27it/s]\u001b[A\nIter (loss=3.776):  15%|█▌        | 74/478 [00:58<05:15,  1.28it/s]\u001b[A\nIter (loss=4.248):  15%|█▌        | 74/478 [00:59<05:15,  1.28it/s]\u001b[A\nIter (loss=4.248):  16%|█▌        | 75/478 [00:59<05:14,  1.28it/s]\u001b[A\nIter (loss=3.799):  16%|█▌        | 75/478 [00:59<05:14,  1.28it/s]\u001b[A\nIter (loss=3.799):  16%|█▌        | 76/478 [01:00<05:12,  1.29it/s]\u001b[A\nIter (loss=4.019):  16%|█▌        | 76/478 [01:00<05:12,  1.29it/s]\u001b[A\nIter (loss=4.019):  16%|█▌        | 77/478 [01:00<05:10,  1.29it/s]\u001b[A\nIter (loss=3.867):  16%|█▌        | 77/478 [01:01<05:10,  1.29it/s]\u001b[A\nIter (loss=3.867):  16%|█▋        | 78/478 [01:01<05:11,  1.29it/s]\u001b[A\nIter (loss=3.744):  16%|█▋        | 78/478 [01:02<05:11,  1.29it/s]\u001b[A\nIter (loss=3.744):  17%|█▋        | 79/478 [01:02<05:08,  1.29it/s]\u001b[A\nIter (loss=3.709):  17%|█▋        | 79/478 [01:02<05:08,  1.29it/s]\u001b[A\nIter (loss=3.709):  17%|█▋        | 80/478 [01:03<05:06,  1.30it/s]\u001b[A\nIter (loss=3.813):  17%|█▋        | 80/478 [01:03<05:06,  1.30it/s]\u001b[A\nIter (loss=3.813):  17%|█▋        | 81/478 [01:03<05:08,  1.29it/s]\u001b[A\nIter (loss=3.961):  17%|█▋        | 81/478 [01:04<05:08,  1.29it/s]\u001b[A\nIter (loss=3.961):  17%|█▋        | 82/478 [01:04<05:08,  1.28it/s]\u001b[A\nIter (loss=3.899):  17%|█▋        | 82/478 [01:05<05:08,  1.28it/s]\u001b[A\nIter (loss=3.899):  17%|█▋        | 83/478 [01:05<05:06,  1.29it/s]\u001b[A\nIter (loss=3.851):  17%|█▋        | 83/478 [01:06<05:06,  1.29it/s]\u001b[A\nIter (loss=3.851):  18%|█▊        | 84/478 [01:06<05:08,  1.28it/s]\u001b[A\nIter (loss=4.061):  18%|█▊        | 84/478 [01:06<05:08,  1.28it/s]\u001b[A\nIter (loss=4.061):  18%|█▊        | 85/478 [01:07<05:07,  1.28it/s]\u001b[A\nIter (loss=4.227):  18%|█▊        | 85/478 [01:07<05:07,  1.28it/s]\u001b[A\nIter (loss=4.227):  18%|█▊        | 86/478 [01:07<05:03,  1.29it/s]\u001b[A\nIter (loss=3.689):  18%|█▊        | 86/478 [01:08<05:03,  1.29it/s]\u001b[A\nIter (loss=3.689):  18%|█▊        | 87/478 [01:08<05:07,  1.27it/s]\u001b[A\nIter (loss=3.637):  18%|█▊        | 87/478 [01:09<05:07,  1.27it/s]\u001b[A\nIter (loss=3.637):  18%|█▊        | 88/478 [01:09<05:13,  1.25it/s]\u001b[A\nIter (loss=3.826):  18%|█▊        | 88/478 [01:10<05:13,  1.25it/s]\u001b[A\nIter (loss=3.826):  19%|█▊        | 89/478 [01:10<05:09,  1.26it/s]\u001b[A\nIter (loss=3.613):  19%|█▊        | 89/478 [01:10<05:09,  1.26it/s]\u001b[A\nIter (loss=3.613):  19%|█▉        | 90/478 [01:11<05:06,  1.26it/s]\u001b[A\nIter (loss=4.182):  19%|█▉        | 90/478 [01:11<05:06,  1.26it/s]\u001b[A\nIter (loss=4.182):  19%|█▉        | 91/478 [01:11<05:02,  1.28it/s]\u001b[A\nIter (loss=3.907):  19%|█▉        | 91/478 [01:12<05:02,  1.28it/s]\u001b[A\nIter (loss=3.907):  19%|█▉        | 92/478 [01:12<04:57,  1.30it/s]\u001b[A\nIter (loss=3.763):  19%|█▉        | 92/478 [01:13<04:57,  1.30it/s]\u001b[A\nIter (loss=3.763):  19%|█▉        | 93/478 [01:13<04:58,  1.29it/s]\u001b[A\nIter (loss=3.816):  19%|█▉        | 93/478 [01:13<04:58,  1.29it/s]\u001b[A\nIter (loss=3.816):  20%|█▉        | 94/478 [01:14<05:02,  1.27it/s]\u001b[A\nIter (loss=3.599):  20%|█▉        | 94/478 [01:14<05:02,  1.27it/s]\u001b[A\nIter (loss=3.599):  20%|█▉        | 95/478 [01:14<05:00,  1.28it/s]\u001b[A\nIter (loss=3.753):  20%|█▉        | 95/478 [01:15<05:00,  1.28it/s]\u001b[A\nIter (loss=3.753):  20%|██        | 96/478 [01:15<04:57,  1.28it/s]\u001b[A\nIter (loss=3.704):  20%|██        | 96/478 [01:16<04:57,  1.28it/s]\u001b[A\nIter (loss=3.704):  20%|██        | 97/478 [01:16<05:00,  1.27it/s]\u001b[A\nIter (loss=3.619):  20%|██        | 97/478 [01:17<05:00,  1.27it/s]\u001b[A\nIter (loss=3.619):  21%|██        | 98/478 [01:17<05:01,  1.26it/s]\u001b[A\nIter (loss=4.063):  21%|██        | 98/478 [01:17<05:01,  1.26it/s]\u001b[A\nIter (loss=4.063):  21%|██        | 99/478 [01:18<04:58,  1.27it/s]\u001b[A\nIter (loss=4.025):  21%|██        | 99/478 [01:18<04:58,  1.27it/s]\u001b[A\nIter (loss=4.025):  21%|██        | 100/478 [01:18<04:54,  1.28it/s]\u001b[A\nIter (loss=3.432):  21%|██        | 100/478 [01:19<04:54,  1.28it/s]\u001b[A\nIter (loss=3.432):  21%|██        | 101/478 [01:19<04:51,  1.29it/s]\u001b[A\nIter (loss=3.549):  21%|██        | 101/478 [01:20<04:51,  1.29it/s]\u001b[A\nIter (loss=3.549):  21%|██▏       | 102/478 [01:20<04:50,  1.29it/s]\u001b[A\nIter (loss=3.598):  21%|██▏       | 102/478 [01:20<04:50,  1.29it/s]\u001b[A\nIter (loss=3.598):  22%|██▏       | 103/478 [01:21<04:53,  1.28it/s]\u001b[A\nIter (loss=4.059):  22%|██▏       | 103/478 [01:21<04:53,  1.28it/s]\u001b[A\nIter (loss=4.059):  22%|██▏       | 104/478 [01:21<04:52,  1.28it/s]\u001b[A\nIter (loss=3.460):  22%|██▏       | 104/478 [01:22<04:52,  1.28it/s]\u001b[A\nIter (loss=3.460):  22%|██▏       | 105/478 [01:22<04:54,  1.26it/s]\u001b[A\nIter (loss=3.510):  22%|██▏       | 105/478 [01:23<04:54,  1.26it/s]\u001b[A\nIter (loss=3.510):  22%|██▏       | 106/478 [01:23<04:51,  1.28it/s]\u001b[A\nIter (loss=4.031):  22%|██▏       | 106/478 [01:24<04:51,  1.28it/s]\u001b[A\nIter (loss=4.031):  22%|██▏       | 107/478 [01:24<04:47,  1.29it/s]\u001b[A\nIter (loss=3.825):  22%|██▏       | 107/478 [01:24<04:47,  1.29it/s]\u001b[A\nIter (loss=3.825):  23%|██▎       | 108/478 [01:25<04:45,  1.30it/s]\u001b[A\nIter (loss=3.648):  23%|██▎       | 108/478 [01:25<04:45,  1.30it/s]\u001b[A\nIter (loss=3.648):  23%|██▎       | 109/478 [01:25<04:44,  1.30it/s]\u001b[A\nIter (loss=4.013):  23%|██▎       | 109/478 [01:26<04:44,  1.30it/s]\u001b[A\nIter (loss=4.013):  23%|██▎       | 110/478 [01:26<04:42,  1.30it/s]\u001b[A\nIter (loss=3.873):  23%|██▎       | 110/478 [01:27<04:42,  1.30it/s]\u001b[A\nIter (loss=3.873):  23%|██▎       | 111/478 [01:27<04:41,  1.30it/s]\u001b[A\nIter (loss=3.772):  23%|██▎       | 111/478 [01:27<04:41,  1.30it/s]\u001b[A\nIter (loss=3.772):  23%|██▎       | 112/478 [01:28<04:41,  1.30it/s]\u001b[A\nIter (loss=4.244):  23%|██▎       | 112/478 [01:28<04:41,  1.30it/s]\u001b[A\nIter (loss=4.244):  24%|██▎       | 113/478 [01:28<04:37,  1.31it/s]\u001b[A\nIter (loss=3.843):  24%|██▎       | 113/478 [01:29<04:37,  1.31it/s]\u001b[A\nIter (loss=3.843):  24%|██▍       | 114/478 [01:29<04:37,  1.31it/s]\u001b[A\nIter (loss=3.771):  24%|██▍       | 114/478 [01:30<04:37,  1.31it/s]\u001b[A\nIter (loss=3.771):  24%|██▍       | 115/478 [01:30<04:42,  1.29it/s]\u001b[A\nIter (loss=3.865):  24%|██▍       | 115/478 [01:30<04:42,  1.29it/s]\u001b[A\nIter (loss=3.865):  24%|██▍       | 116/478 [01:31<04:38,  1.30it/s]\u001b[A\nIter (loss=3.880):  24%|██▍       | 116/478 [01:31<04:38,  1.30it/s]\u001b[A\nIter (loss=3.880):  24%|██▍       | 117/478 [01:31<04:40,  1.29it/s]\u001b[A\nIter (loss=3.577):  24%|██▍       | 117/478 [01:32<04:40,  1.29it/s]\u001b[A\nIter (loss=3.577):  25%|██▍       | 118/478 [01:32<04:38,  1.29it/s]\u001b[A\nIter (loss=3.998):  25%|██▍       | 118/478 [01:33<04:38,  1.29it/s]\u001b[A\nIter (loss=3.998):  25%|██▍       | 119/478 [01:33<04:36,  1.30it/s]\u001b[A\nIter (loss=3.911):  25%|██▍       | 119/478 [01:34<04:36,  1.30it/s]\u001b[A\nIter (loss=3.911):  25%|██▌       | 120/478 [01:34<04:33,  1.31it/s]\u001b[A\nIter (loss=3.584):  25%|██▌       | 120/478 [01:34<04:33,  1.31it/s]\u001b[A\nIter (loss=3.584):  25%|██▌       | 121/478 [01:35<04:30,  1.32it/s]\u001b[A\nIter (loss=4.009):  25%|██▌       | 121/478 [01:35<04:30,  1.32it/s]\u001b[A\nIter (loss=4.009):  26%|██▌       | 122/478 [01:35<04:31,  1.31it/s]\u001b[A\nIter (loss=4.175):  26%|██▌       | 122/478 [01:36<04:31,  1.31it/s]\u001b[A\nIter (loss=4.175):  26%|██▌       | 123/478 [01:36<04:38,  1.27it/s]\u001b[A\nIter (loss=3.613):  26%|██▌       | 123/478 [01:37<04:38,  1.27it/s]\u001b[A\nIter (loss=3.613):  26%|██▌       | 124/478 [01:37<04:38,  1.27it/s]\u001b[A\nIter (loss=3.572):  26%|██▌       | 124/478 [01:37<04:38,  1.27it/s]\u001b[A\nIter (loss=3.572):  26%|██▌       | 125/478 [01:38<04:38,  1.27it/s]\u001b[A\nIter (loss=3.670):  26%|██▌       | 125/478 [01:38<04:38,  1.27it/s]\u001b[A\nIter (loss=3.670):  26%|██▋       | 126/478 [01:38<04:36,  1.27it/s]\u001b[A\nIter (loss=3.500):  26%|██▋       | 126/478 [01:39<04:36,  1.27it/s]\u001b[A\nIter (loss=3.500):  27%|██▋       | 127/478 [01:39<04:30,  1.30it/s]\u001b[A\nIter (loss=4.038):  27%|██▋       | 127/478 [01:40<04:30,  1.30it/s]\u001b[A\nIter (loss=4.038):  27%|██▋       | 128/478 [01:40<04:33,  1.28it/s]\u001b[A\nIter (loss=4.102):  27%|██▋       | 128/478 [01:41<04:33,  1.28it/s]\u001b[A\nIter (loss=4.102):  27%|██▋       | 129/478 [01:41<04:33,  1.28it/s]\u001b[A\nIter (loss=4.246):  27%|██▋       | 129/478 [01:41<04:33,  1.28it/s]\u001b[A\nIter (loss=4.246):  27%|██▋       | 130/478 [01:42<04:35,  1.26it/s]\u001b[A\nIter (loss=3.953):  27%|██▋       | 130/478 [01:42<04:35,  1.26it/s]\u001b[A\nIter (loss=3.953):  27%|██▋       | 131/478 [01:42<04:32,  1.27it/s]\u001b[A\nIter (loss=4.137):  27%|██▋       | 131/478 [01:43<04:32,  1.27it/s]\u001b[A\nIter (loss=4.137):  28%|██▊       | 132/478 [01:43<04:29,  1.28it/s]\u001b[A\nIter (loss=3.987):  28%|██▊       | 132/478 [01:44<04:29,  1.28it/s]\u001b[A\nIter (loss=3.987):  28%|██▊       | 133/478 [01:44<04:37,  1.24it/s]\u001b[A\nIter (loss=4.877):  28%|██▊       | 133/478 [01:45<04:37,  1.24it/s]\u001b[A\nIter (loss=4.877):  28%|██▊       | 134/478 [01:45<04:34,  1.26it/s]\u001b[A\nIter (loss=4.536):  28%|██▊       | 134/478 [01:45<04:34,  1.26it/s]\u001b[A\nIter (loss=4.536):  28%|██▊       | 135/478 [01:46<04:36,  1.24it/s]\u001b[A\nIter (loss=4.122):  28%|██▊       | 135/478 [01:46<04:36,  1.24it/s]\u001b[A\nIter (loss=4.122):  28%|██▊       | 136/478 [01:46<04:35,  1.24it/s]\u001b[A\nIter (loss=3.916):  28%|██▊       | 136/478 [01:47<04:35,  1.24it/s]\u001b[A\nIter (loss=3.916):  29%|██▊       | 137/478 [01:47<04:30,  1.26it/s]\u001b[A\nIter (loss=4.059):  29%|██▊       | 137/478 [01:48<04:30,  1.26it/s]\u001b[A\nIter (loss=4.059):  29%|██▉       | 138/478 [01:48<04:26,  1.28it/s]\u001b[A\nIter (loss=3.998):  29%|██▉       | 138/478 [01:48<04:26,  1.28it/s]\u001b[A\nIter (loss=3.998):  29%|██▉       | 139/478 [01:49<04:24,  1.28it/s]\u001b[A\nIter (loss=4.123):  29%|██▉       | 139/478 [01:49<04:24,  1.28it/s]\u001b[A\nIter (loss=4.123):  29%|██▉       | 140/478 [01:49<04:20,  1.30it/s]\u001b[A\nIter (loss=4.309):  29%|██▉       | 140/478 [01:50<04:20,  1.30it/s]\u001b[A\nIter (loss=4.309):  29%|██▉       | 141/478 [01:50<04:21,  1.29it/s]\u001b[A\nIter (loss=4.011):  29%|██▉       | 141/478 [01:51<04:21,  1.29it/s]\u001b[A\nIter (loss=4.011):  30%|██▉       | 142/478 [01:51<04:22,  1.28it/s]\u001b[A\nIter (loss=3.845):  30%|██▉       | 142/478 [01:52<04:22,  1.28it/s]\u001b[A\nIter (loss=3.845):  30%|██▉       | 143/478 [01:52<04:20,  1.29it/s]\u001b[A\nIter (loss=4.089):  30%|██▉       | 143/478 [01:52<04:20,  1.29it/s]\u001b[A\nIter (loss=4.089):  30%|███       | 144/478 [01:53<04:17,  1.30it/s]\u001b[A\nIter (loss=4.073):  30%|███       | 144/478 [01:53<04:17,  1.30it/s]\u001b[A\nIter (loss=4.073):  30%|███       | 145/478 [01:53<04:18,  1.29it/s]\u001b[A\nIter (loss=4.475):  30%|███       | 145/478 [01:54<04:18,  1.29it/s]\u001b[A\nIter (loss=4.475):  31%|███       | 146/478 [01:54<04:19,  1.28it/s]\u001b[A\nIter (loss=3.865):  31%|███       | 146/478 [01:55<04:19,  1.28it/s]\u001b[A\nIter (loss=3.865):  31%|███       | 147/478 [01:55<04:20,  1.27it/s]\u001b[A\nIter (loss=3.645):  31%|███       | 147/478 [01:56<04:20,  1.27it/s]\u001b[A\nIter (loss=3.645):  31%|███       | 148/478 [01:56<04:19,  1.27it/s]\u001b[A\nIter (loss=3.858):  31%|███       | 148/478 [01:56<04:19,  1.27it/s]\u001b[A\nIter (loss=3.858):  31%|███       | 149/478 [01:57<04:15,  1.29it/s]\u001b[A\nIter (loss=3.917):  31%|███       | 149/478 [01:57<04:15,  1.29it/s]\u001b[A\nIter (loss=3.917):  31%|███▏      | 150/478 [01:57<04:15,  1.28it/s]\u001b[A\nIter (loss=3.845):  31%|███▏      | 150/478 [01:58<04:15,  1.28it/s]\u001b[A\nIter (loss=3.845):  32%|███▏      | 151/478 [01:58<04:12,  1.30it/s]\u001b[A\nIter (loss=3.917):  32%|███▏      | 151/478 [01:59<04:12,  1.30it/s]\u001b[A\nIter (loss=3.917):  32%|███▏      | 152/478 [01:59<04:09,  1.31it/s]\u001b[A\nIter (loss=4.101):  32%|███▏      | 152/478 [01:59<04:09,  1.31it/s]\u001b[A\nIter (loss=4.101):  32%|███▏      | 153/478 [02:00<04:09,  1.31it/s]\u001b[A\nIter (loss=3.677):  32%|███▏      | 153/478 [02:00<04:09,  1.31it/s]\u001b[A\nIter (loss=3.677):  32%|███▏      | 154/478 [02:00<04:10,  1.29it/s]\u001b[A\nIter (loss=3.735):  32%|███▏      | 154/478 [02:01<04:10,  1.29it/s]\u001b[A\nIter (loss=3.735):  32%|███▏      | 155/478 [02:01<04:09,  1.30it/s]\u001b[A\nIter (loss=3.921):  32%|███▏      | 155/478 [02:02<04:09,  1.30it/s]\u001b[A\nIter (loss=3.921):  33%|███▎      | 156/478 [02:02<04:09,  1.29it/s]\u001b[A\nIter (loss=3.596):  33%|███▎      | 156/478 [02:02<04:09,  1.29it/s]\u001b[A\nIter (loss=3.596):  33%|███▎      | 157/478 [02:03<04:08,  1.29it/s]\u001b[A\nIter (loss=3.725):  33%|███▎      | 157/478 [02:03<04:08,  1.29it/s]\u001b[A\nIter (loss=3.725):  33%|███▎      | 158/478 [02:03<04:08,  1.29it/s]\u001b[A\nIter (loss=3.913):  33%|███▎      | 158/478 [02:04<04:08,  1.29it/s]\u001b[A\nIter (loss=3.913):  33%|███▎      | 159/478 [02:04<04:08,  1.28it/s]\u001b[A\nIter (loss=3.374):  33%|███▎      | 159/478 [02:05<04:08,  1.28it/s]\u001b[A\nIter (loss=3.374):  33%|███▎      | 160/478 [02:05<04:07,  1.29it/s]\u001b[A\nIter (loss=3.883):  33%|███▎      | 160/478 [02:06<04:07,  1.29it/s]\u001b[A\nIter (loss=3.883):  34%|███▎      | 161/478 [02:06<04:05,  1.29it/s]\u001b[A\nIter (loss=4.410):  34%|███▎      | 161/478 [02:06<04:05,  1.29it/s]\u001b[A\nIter (loss=4.410):  34%|███▍      | 162/478 [02:07<04:03,  1.30it/s]\u001b[A\nIter (loss=4.133):  34%|███▍      | 162/478 [02:07<04:03,  1.30it/s]\u001b[A\nIter (loss=4.133):  34%|███▍      | 163/478 [02:07<04:02,  1.30it/s]\u001b[A\nIter (loss=3.600):  34%|███▍      | 163/478 [02:08<04:02,  1.30it/s]\u001b[A\nIter (loss=3.600):  34%|███▍      | 164/478 [02:08<04:02,  1.30it/s]\u001b[A\nIter (loss=4.132):  34%|███▍      | 164/478 [02:09<04:02,  1.30it/s]\u001b[A\nIter (loss=4.132):  35%|███▍      | 165/478 [02:09<04:00,  1.30it/s]\u001b[A\nIter (loss=3.776):  35%|███▍      | 165/478 [02:09<04:00,  1.30it/s]\u001b[A\nIter (loss=3.776):  35%|███▍      | 166/478 [02:10<04:00,  1.30it/s]\u001b[A\nIter (loss=3.675):  35%|███▍      | 166/478 [02:10<04:00,  1.30it/s]\u001b[A\nIter (loss=3.675):  35%|███▍      | 167/478 [02:10<03:59,  1.30it/s]\u001b[A\nIter (loss=3.937):  35%|███▍      | 167/478 [02:11<03:59,  1.30it/s]\u001b[A\nIter (loss=3.937):  35%|███▌      | 168/478 [02:11<03:59,  1.29it/s]\u001b[A\nIter (loss=3.833):  35%|███▌      | 168/478 [02:12<03:59,  1.29it/s]\u001b[A\nIter (loss=3.833):  35%|███▌      | 169/478 [02:12<03:58,  1.29it/s]\u001b[A\nIter (loss=3.898):  35%|███▌      | 169/478 [02:12<03:58,  1.29it/s]\u001b[A\nIter (loss=3.898):  36%|███▌      | 170/478 [02:13<03:58,  1.29it/s]\u001b[A\nIter (loss=3.843):  36%|███▌      | 170/478 [02:13<03:58,  1.29it/s]\u001b[A\nIter (loss=3.843):  36%|███▌      | 171/478 [02:14<03:57,  1.29it/s]\u001b[A\nIter (loss=3.771):  36%|███▌      | 171/478 [02:14<03:57,  1.29it/s]\u001b[A\nIter (loss=3.771):  36%|███▌      | 172/478 [02:14<03:55,  1.30it/s]\u001b[A\nIter (loss=3.689):  36%|███▌      | 172/478 [02:15<03:55,  1.30it/s]\u001b[A\nIter (loss=3.689):  36%|███▌      | 173/478 [02:15<03:51,  1.32it/s]\u001b[A\nIter (loss=4.090):  36%|███▌      | 173/478 [02:16<03:51,  1.32it/s]\u001b[A\nIter (loss=4.090):  36%|███▋      | 174/478 [02:16<03:55,  1.29it/s]\u001b[A\nIter (loss=3.796):  36%|███▋      | 174/478 [02:16<03:55,  1.29it/s]\u001b[A\nIter (loss=3.796):  37%|███▋      | 175/478 [02:17<03:55,  1.29it/s]\u001b[A\nIter (loss=3.691):  37%|███▋      | 175/478 [02:17<03:55,  1.29it/s]\u001b[A\nIter (loss=3.691):  37%|███▋      | 176/478 [02:17<03:52,  1.30it/s]\u001b[A\nIter (loss=3.494):  37%|███▋      | 176/478 [02:18<03:52,  1.30it/s]\u001b[A\nIter (loss=3.494):  37%|███▋      | 177/478 [02:18<03:50,  1.30it/s]\u001b[A\nIter (loss=4.377):  37%|███▋      | 177/478 [02:19<03:50,  1.30it/s]\u001b[A\nIter (loss=4.377):  37%|███▋      | 178/478 [02:19<03:48,  1.31it/s]\u001b[A\nIter (loss=4.223):  37%|███▋      | 178/478 [02:19<03:48,  1.31it/s]\u001b[A\nIter (loss=4.223):  37%|███▋      | 179/478 [02:20<03:50,  1.30it/s]\u001b[A\nIter (loss=3.707):  37%|███▋      | 179/478 [02:20<03:50,  1.30it/s]\u001b[A\nIter (loss=3.707):  38%|███▊      | 180/478 [02:20<03:49,  1.30it/s]\u001b[A\nIter (loss=3.984):  38%|███▊      | 180/478 [02:21<03:49,  1.30it/s]\u001b[A\nIter (loss=3.984):  38%|███▊      | 181/478 [02:21<03:49,  1.29it/s]\u001b[A\nIter (loss=3.884):  38%|███▊      | 181/478 [02:22<03:49,  1.29it/s]\u001b[A\nIter (loss=3.884):  38%|███▊      | 182/478 [02:22<03:48,  1.29it/s]\u001b[A\nIter (loss=3.494):  38%|███▊      | 182/478 [02:22<03:48,  1.29it/s]\u001b[A\nIter (loss=3.494):  38%|███▊      | 183/478 [02:23<03:48,  1.29it/s]\u001b[A\nIter (loss=3.957):  38%|███▊      | 183/478 [02:23<03:48,  1.29it/s]\u001b[A\nIter (loss=3.957):  38%|███▊      | 184/478 [02:24<03:46,  1.30it/s]\u001b[A\nIter (loss=3.689):  38%|███▊      | 184/478 [02:24<03:46,  1.30it/s]\u001b[A\nIter (loss=3.689):  39%|███▊      | 185/478 [02:24<03:46,  1.30it/s]\u001b[A\nIter (loss=3.803):  39%|███▊      | 185/478 [02:25<03:46,  1.30it/s]\u001b[A\nIter (loss=3.803):  39%|███▉      | 186/478 [02:25<03:47,  1.29it/s]\u001b[A\nIter (loss=4.056):  39%|███▉      | 186/478 [02:26<03:47,  1.29it/s]\u001b[A\nIter (loss=4.056):  39%|███▉      | 187/478 [02:26<03:47,  1.28it/s]\u001b[A\nIter (loss=3.436):  39%|███▉      | 187/478 [02:26<03:47,  1.28it/s]\u001b[A\nIter (loss=3.436):  39%|███▉      | 188/478 [02:27<03:45,  1.29it/s]\u001b[A\nIter (loss=4.056):  39%|███▉      | 188/478 [02:27<03:45,  1.29it/s]\u001b[A\nIter (loss=4.056):  40%|███▉      | 189/478 [02:27<03:44,  1.29it/s]\u001b[A\nIter (loss=4.093):  40%|███▉      | 189/478 [02:28<03:44,  1.29it/s]\u001b[A\nIter (loss=4.093):  40%|███▉      | 190/478 [02:28<03:43,  1.29it/s]\u001b[A\nIter (loss=3.779):  40%|███▉      | 190/478 [02:29<03:43,  1.29it/s]\u001b[A\nIter (loss=3.779):  40%|███▉      | 191/478 [02:29<03:41,  1.29it/s]\u001b[A\nIter (loss=3.795):  40%|███▉      | 191/478 [02:29<03:41,  1.29it/s]\u001b[A\nIter (loss=3.795):  40%|████      | 192/478 [02:30<03:39,  1.31it/s]\u001b[A\nIter (loss=3.627):  40%|████      | 192/478 [02:30<03:39,  1.31it/s]\u001b[A\nIter (loss=3.627):  40%|████      | 193/478 [02:30<03:40,  1.29it/s]\u001b[A\nIter (loss=3.856):  40%|████      | 193/478 [02:31<03:40,  1.29it/s]\u001b[A\nIter (loss=3.856):  41%|████      | 194/478 [02:31<03:38,  1.30it/s]\u001b[A\nIter (loss=3.906):  41%|████      | 194/478 [02:32<03:38,  1.30it/s]\u001b[A\nIter (loss=3.906):  41%|████      | 195/478 [02:32<03:38,  1.29it/s]\u001b[A\nIter (loss=3.842):  41%|████      | 195/478 [02:33<03:38,  1.29it/s]\u001b[A\nIter (loss=3.842):  41%|████      | 196/478 [02:33<03:37,  1.30it/s]\u001b[A\nIter (loss=3.996):  41%|████      | 196/478 [02:33<03:37,  1.30it/s]\u001b[A\nIter (loss=3.996):  41%|████      | 197/478 [02:34<03:39,  1.28it/s]\u001b[A\nIter (loss=4.156):  41%|████      | 197/478 [02:34<03:39,  1.28it/s]\u001b[A\nIter (loss=4.156):  41%|████▏     | 198/478 [02:34<03:37,  1.29it/s]\u001b[A\nIter (loss=3.864):  41%|████▏     | 198/478 [02:35<03:37,  1.29it/s]\u001b[A\nIter (loss=3.864):  42%|████▏     | 199/478 [02:35<03:34,  1.30it/s]\u001b[A\nIter (loss=3.893):  42%|████▏     | 199/478 [02:36<03:34,  1.30it/s]\u001b[A\nIter (loss=3.893):  42%|████▏     | 200/478 [02:36<03:33,  1.30it/s]\u001b[A\nIter (loss=3.950):  42%|████▏     | 200/478 [02:36<03:33,  1.30it/s]\u001b[A\nIter (loss=3.950):  42%|████▏     | 201/478 [02:37<03:31,  1.31it/s]\u001b[A\nIter (loss=3.923):  42%|████▏     | 201/478 [02:37<03:31,  1.31it/s]\u001b[A\nIter (loss=3.923):  42%|████▏     | 202/478 [02:37<03:32,  1.30it/s]\u001b[A\nIter (loss=4.044):  42%|████▏     | 202/478 [02:38<03:32,  1.30it/s]\u001b[A\nIter (loss=4.044):  42%|████▏     | 203/478 [02:38<03:31,  1.30it/s]\u001b[A\nIter (loss=3.806):  42%|████▏     | 203/478 [02:39<03:31,  1.30it/s]\u001b[A\nIter (loss=3.806):  43%|████▎     | 204/478 [02:39<03:29,  1.31it/s]\u001b[A\nIter (loss=3.926):  43%|████▎     | 204/478 [02:39<03:29,  1.31it/s]\u001b[A\nIter (loss=3.926):  43%|████▎     | 205/478 [02:40<03:29,  1.30it/s]\u001b[A\nIter (loss=4.012):  43%|████▎     | 205/478 [02:40<03:29,  1.30it/s]\u001b[A\nIter (loss=4.012):  43%|████▎     | 206/478 [02:41<03:32,  1.28it/s]\u001b[A\nIter (loss=3.784):  43%|████▎     | 206/478 [02:41<03:32,  1.28it/s]\u001b[A\nIter (loss=3.784):  43%|████▎     | 207/478 [02:41<03:33,  1.27it/s]\u001b[A\nIter (loss=3.429):  43%|████▎     | 207/478 [02:42<03:33,  1.27it/s]\u001b[A\nIter (loss=3.429):  44%|████▎     | 208/478 [02:42<03:33,  1.27it/s]\u001b[A\nIter (loss=3.850):  44%|████▎     | 208/478 [02:43<03:33,  1.27it/s]\u001b[A\nIter (loss=3.850):  44%|████▎     | 209/478 [02:43<03:34,  1.26it/s]\u001b[A\nIter (loss=3.940):  44%|████▎     | 209/478 [02:43<03:34,  1.26it/s]\u001b[A\nIter (loss=3.940):  44%|████▍     | 210/478 [02:44<03:32,  1.26it/s]\u001b[A\nIter (loss=3.795):  44%|████▍     | 210/478 [02:44<03:32,  1.26it/s]\u001b[A\nIter (loss=3.795):  44%|████▍     | 211/478 [02:45<03:31,  1.26it/s]\u001b[A\nIter (loss=4.036):  44%|████▍     | 211/478 [02:45<03:31,  1.26it/s]\u001b[A\nIter (loss=4.036):  44%|████▍     | 212/478 [02:45<03:32,  1.25it/s]\u001b[A\nIter (loss=3.724):  44%|████▍     | 212/478 [02:46<03:32,  1.25it/s]\u001b[A\nIter (loss=3.724):  45%|████▍     | 213/478 [02:46<03:31,  1.25it/s]\u001b[A\nIter (loss=3.761):  45%|████▍     | 213/478 [02:47<03:31,  1.25it/s]\u001b[A\nIter (loss=3.761):  45%|████▍     | 214/478 [02:47<03:29,  1.26it/s]\u001b[A\nIter (loss=3.804):  45%|████▍     | 214/478 [02:47<03:29,  1.26it/s]\u001b[A\nIter (loss=3.804):  45%|████▍     | 215/478 [02:48<03:27,  1.27it/s]\u001b[A\nIter (loss=3.547):  45%|████▍     | 215/478 [02:48<03:27,  1.27it/s]\u001b[A\nIter (loss=3.547):  45%|████▌     | 216/478 [02:48<03:23,  1.29it/s]\u001b[A\nIter (loss=3.436):  45%|████▌     | 216/478 [02:49<03:23,  1.29it/s]\u001b[A\nIter (loss=3.436):  45%|████▌     | 217/478 [02:49<03:20,  1.30it/s]\u001b[A\nIter (loss=4.115):  45%|████▌     | 217/478 [02:50<03:20,  1.30it/s]\u001b[A\nIter (loss=4.115):  46%|████▌     | 218/478 [02:50<03:19,  1.30it/s]\u001b[A\nIter (loss=3.986):  46%|████▌     | 218/478 [02:50<03:19,  1.30it/s]\u001b[A\nIter (loss=3.986):  46%|████▌     | 219/478 [02:51<03:19,  1.30it/s]\u001b[A\nIter (loss=3.697):  46%|████▌     | 219/478 [02:51<03:19,  1.30it/s]\u001b[A\nIter (loss=3.697):  46%|████▌     | 220/478 [02:52<03:19,  1.29it/s]\u001b[A\nIter (loss=3.532):  46%|████▌     | 220/478 [02:52<03:19,  1.29it/s]\u001b[A\nIter (loss=3.532):  46%|████▌     | 221/478 [02:52<03:21,  1.27it/s]\u001b[A\nIter (loss=3.715):  46%|████▌     | 221/478 [02:53<03:21,  1.27it/s]\u001b[A\nIter (loss=3.715):  46%|████▋     | 222/478 [02:53<03:19,  1.29it/s]\u001b[A\nIter (loss=3.749):  46%|████▋     | 222/478 [02:54<03:19,  1.29it/s]\u001b[A\nIter (loss=3.749):  47%|████▋     | 223/478 [02:54<03:17,  1.29it/s]\u001b[A\nIter (loss=3.964):  47%|████▋     | 223/478 [02:54<03:17,  1.29it/s]\u001b[A\nIter (loss=3.964):  47%|████▋     | 224/478 [02:55<03:15,  1.30it/s]\u001b[A\nIter (loss=3.775):  47%|████▋     | 224/478 [02:55<03:15,  1.30it/s]\u001b[A\nIter (loss=3.775):  47%|████▋     | 225/478 [02:55<03:15,  1.30it/s]\u001b[A\nIter (loss=3.545):  47%|████▋     | 225/478 [02:56<03:15,  1.30it/s]\u001b[A\nIter (loss=3.545):  47%|████▋     | 226/478 [02:56<03:15,  1.29it/s]\u001b[A\nIter (loss=4.176):  47%|████▋     | 226/478 [02:57<03:15,  1.29it/s]\u001b[A\nIter (loss=4.176):  47%|████▋     | 227/478 [02:57<03:16,  1.28it/s]\u001b[A\nIter (loss=4.121):  47%|████▋     | 227/478 [02:57<03:16,  1.28it/s]\u001b[A\nIter (loss=4.121):  48%|████▊     | 228/478 [02:58<03:14,  1.29it/s]\u001b[A\nIter (loss=3.562):  48%|████▊     | 228/478 [02:58<03:14,  1.29it/s]\u001b[A\nIter (loss=3.562):  48%|████▊     | 229/478 [02:59<03:12,  1.29it/s]\u001b[A\nIter (loss=3.845):  48%|████▊     | 229/478 [02:59<03:12,  1.29it/s]\u001b[A\nIter (loss=3.845):  48%|████▊     | 230/478 [02:59<03:11,  1.29it/s]\u001b[A\nIter (loss=3.710):  48%|████▊     | 230/478 [03:00<03:11,  1.29it/s]\u001b[A\nIter (loss=3.710):  48%|████▊     | 231/478 [03:00<03:08,  1.31it/s]\u001b[A\nIter (loss=3.604):  48%|████▊     | 231/478 [03:01<03:08,  1.31it/s]\u001b[A\nIter (loss=3.604):  49%|████▊     | 232/478 [03:01<03:09,  1.30it/s]\u001b[A\nIter (loss=3.750):  49%|████▊     | 232/478 [03:01<03:09,  1.30it/s]\u001b[A\nIter (loss=3.750):  49%|████▊     | 233/478 [03:02<03:08,  1.30it/s]\u001b[A\nIter (loss=3.597):  49%|████▊     | 233/478 [03:02<03:08,  1.30it/s]\u001b[A\nIter (loss=3.597):  49%|████▉     | 234/478 [03:02<03:07,  1.30it/s]\u001b[A\nIter (loss=3.919):  49%|████▉     | 234/478 [03:03<03:07,  1.30it/s]\u001b[A\nIter (loss=3.919):  49%|████▉     | 235/478 [03:03<03:06,  1.30it/s]\u001b[A\nIter (loss=3.742):  49%|████▉     | 235/478 [03:04<03:06,  1.30it/s]\u001b[A\nIter (loss=3.742):  49%|████▉     | 236/478 [03:04<03:03,  1.32it/s]\u001b[A\nIter (loss=3.726):  49%|████▉     | 236/478 [03:04<03:03,  1.32it/s]\u001b[A\nIter (loss=3.726):  50%|████▉     | 237/478 [03:05<03:06,  1.29it/s]\u001b[A\nIter (loss=3.541):  50%|████▉     | 237/478 [03:05<03:06,  1.29it/s]\u001b[A\nIter (loss=3.541):  50%|████▉     | 238/478 [03:05<03:10,  1.26it/s]\u001b[A\nIter (loss=3.694):  50%|████▉     | 238/478 [03:06<03:10,  1.26it/s]\u001b[A\nIter (loss=3.694):  50%|█████     | 239/478 [03:06<03:10,  1.25it/s]\u001b[A\nIter (loss=4.076):  50%|█████     | 239/478 [03:07<03:10,  1.25it/s]\u001b[A\nIter (loss=4.076):  50%|█████     | 240/478 [03:07<03:09,  1.26it/s]\u001b[A\nIter (loss=3.643):  50%|█████     | 240/478 [03:08<03:09,  1.26it/s]\u001b[A\nIter (loss=3.643):  50%|█████     | 241/478 [03:08<03:04,  1.28it/s]\u001b[A\nIter (loss=3.341):  50%|█████     | 241/478 [03:08<03:04,  1.28it/s]\u001b[A\nIter (loss=3.341):  51%|█████     | 242/478 [03:09<03:03,  1.28it/s]\u001b[A\nIter (loss=3.879):  51%|█████     | 242/478 [03:09<03:03,  1.28it/s]\u001b[A\nIter (loss=3.879):  51%|█████     | 243/478 [03:09<03:04,  1.28it/s]\u001b[A\nIter (loss=3.661):  51%|█████     | 243/478 [03:10<03:04,  1.28it/s]\u001b[A\nIter (loss=3.661):  51%|█████     | 244/478 [03:10<03:03,  1.28it/s]\u001b[A\nIter (loss=3.879):  51%|█████     | 244/478 [03:11<03:03,  1.28it/s]\u001b[A\nIter (loss=3.879):  51%|█████▏    | 245/478 [03:11<03:01,  1.28it/s]\u001b[A\nIter (loss=3.554):  51%|█████▏    | 245/478 [03:11<03:01,  1.28it/s]\u001b[A\nIter (loss=3.554):  51%|█████▏    | 246/478 [03:12<02:59,  1.29it/s]\u001b[A\nIter (loss=3.708):  51%|█████▏    | 246/478 [03:12<02:59,  1.29it/s]\u001b[A\nIter (loss=3.708):  52%|█████▏    | 247/478 [03:12<02:56,  1.31it/s]\u001b[A\nIter (loss=3.724):  52%|█████▏    | 247/478 [03:13<02:56,  1.31it/s]\u001b[A\nIter (loss=3.724):  52%|█████▏    | 248/478 [03:13<02:56,  1.30it/s]\u001b[A\nIter (loss=3.944):  52%|█████▏    | 248/478 [03:14<02:56,  1.30it/s]\u001b[A\nIter (loss=3.944):  52%|█████▏    | 249/478 [03:14<02:55,  1.30it/s]\u001b[A\nIter (loss=3.568):  52%|█████▏    | 249/478 [03:15<02:55,  1.30it/s]\u001b[A\nIter (loss=3.568):  52%|█████▏    | 250/478 [03:15<02:57,  1.28it/s]\u001b[A\nIter (loss=3.903):  52%|█████▏    | 250/478 [03:15<02:57,  1.28it/s]\u001b[A\nIter (loss=3.903):  53%|█████▎    | 251/478 [03:16<02:56,  1.29it/s]\u001b[A\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"# predict.py","metadata":{}},{"cell_type":"code","source":"import os\nfrom transformers import CLIPProcessor\nfrom torch.utils.data import DataLoader\nimport torch\nimport argparse\nfrom tqdm import tqdm\nimport json\nimport numpy as np\nfrom zipfile import ZipFile\n\n\ndef predict(args, model, device, data, processor, pre = None):\n    data_loader = DataLoader(data, batch_size=args.test_batch_size, collate_fn=MyDataset.collate_func,shuffle=False)\n    results = {}  # Sử dụng dict để lưu kết quả dự đoán\n    index = 0 # Để lưu id của file kết quả\n\n    model.eval()\n    with open(pre,'w',encoding='utf-8') as fout:\n        with torch.no_grad():\n            for i_batch, t_batch in enumerate(data_loader):\n                text_list, image_list, _, id_list = t_batch  # Nhận các phần tử từ batch, bỏ qua label\n                \n                # Xử lý đầu vào cho model\n                inputs = processor(text=text_list, images=image_list, padding='max_length', truncation=True, max_length=args.max_len, return_tensors=\"pt\").to(device)\n                \n                # Dự đoán đầu ra\n                t_outputs = model(inputs, labels=None)\n                predict = torch.argmax(t_outputs[0], -1).cpu().numpy().tolist()\n                \n                for pred in predict:\n                    results[index] = ['not-sarcasm', 'multi-sarcasm', 'text-sarcasm', 'image-sarcasm'][pred]\n                    index += 1\n                \n    # Save predictions to JSON and compress into a zip file\n    with ZipFile(pre, 'w') as zipf:\n        with zipf.open('results.json', 'w') as json_file:\n            json_data = json.dumps({\"results\": results, \"phase\": \"dev\"}, ensure_ascii=False)\n            json_file.write(json_data.encode('utf-8'))\n    \n    print(\"Predictions have been saved to\", pre)      \n\nclass Args:\n    def __init__(self, **entries):\n        self.__dict__.update(entries)\n\ndef main_predict():\n    args_dict = {\n        'device': '0',\n        'max_len': 77,\n        'text_size': 512,\n        'image_size': 768,\n        'dropout_rate': 0,\n        'label_number': 4,\n        'test_batch_size': 32,\n        'model_path': \"/kaggle/working/MV_CLIP\",\n        'save_file': \"B32_HAdata_processed_en_epoch4.zip\",\n        'text_test': '/kaggle/input/text-processed-en/vimmsd-test-processed-en.json',\n        'image_test': '/kaggle/input/muiltimodal-sarcasm/data/public-test-images',\n        'layers': 6,\n        'simple_linear': False,\n        'clip_model': 'openai/clip-vit-base-patch32'\n    }\n    \n    args = Args(**args_dict)\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n    \n    processor = CLIPProcessor.from_pretrained(args.clip_model)\n    model = MV_CLIP(args)\n\n    test_data = MyDataset(args, mode='test', limit=None)\n\n    model.load_state_dict(torch.load('/kaggle/working/MV_CLIP/model4.pt', map_location=\"cpu\"), strict=False)\n    model.to(device)\n    model.eval()\n\n    predict(args, model, device, test_data, processor, pre=args.save_file)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T10:34:29.211654Z","iopub.execute_input":"2024-12-15T10:34:29.212436Z","iopub.status.idle":"2024-12-15T10:34:29.223181Z","shell.execute_reply.started":"2024-12-15T10:34:29.212405Z","shell.execute_reply":"2024-12-15T10:34:29.222327Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"main_predict()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T10:34:29.693608Z","iopub.execute_input":"2024-12-15T10:34:29.693974Z","iopub.status.idle":"2024-12-15T10:34:52.771521Z","shell.execute_reply.started":"2024-12-15T10:34:29.693946Z","shell.execute_reply":"2024-12-15T10:34:52.770522Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_30/2718822845.py:73: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load('/kaggle/working/MV_CLIP/model5.pt', map_location=\"cpu\"), strict=False)\n","output_type":"stream"},{"name":"stdout","text":"Predictions have been saved to B32_vi_epoch5_augment_vi.zip\n","output_type":"stream"}],"execution_count":31}]}